1. Robots.txt handling
2. Crawl depth limiting
3. Request rate limiting
4. Incremental crawling
5. Non HTML media type support
6. Page deduplication (URL canonicalization)
7. And many more!